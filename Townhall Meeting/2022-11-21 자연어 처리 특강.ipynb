{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **토큰 역할 : 문자열을 숫자(토큰)로 변환해줘야 함**\n",
        "- 단어집 큼 --> 모델 훈련에 필요한 메모리 많이 필요\n",
        "- 단어집 작음 --> 모델 훈련에 필요한 메모리 소모 적음[링크 텍스트](https://)"
      ],
      "metadata": {
        "id": "E0tor6IhZREF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0S0vMuQ1XMvi",
        "outputId": "b4d99d33-2240-4f64-960a-67b6115a5c29"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.24.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from transformers import ElectraTokenizer"
      ],
      "metadata": {
        "id": "zW9b-RE7ZXjH"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
        "tokenizer.tokenize(\"[CLS] 한국어 ELECTRA를 공유합니다. [SEP]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuJJa-RZZ5bW",
        "outputId": "2c6c706e-6a8b-417a-b976-b5480107268c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]', '한국어', 'EL', '##EC', '##TRA', '##를', '공유', '##합니다', '.', '[SEP]']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어집 크기\n",
        "tokenizer.vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OibUiyfbUbk",
        "outputId": "7eab81a0-b322-497a-a509-e4196891f6cd"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35000"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer('안녕하세요')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFa2lg_IaIdd",
        "outputId": "d881ace4-8c3e-4843-d4af-1d8d7465aa41"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [2, 11655, 4279, 8553, 3], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer가 모든 자연어를 변환해주진 않는다. ex.뷁뷉녊\n",
        "tokenizer.tokenize(\"뷃뷁녊. 바람이 울고 있어요.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnOLlH49aOVY",
        "outputId": "9cf2804d-8ccb-4ed1-a1a3-4ea7ebad38da"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[UNK]', '.', '바람', '##이', '울', '##고', '있', '##어요', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 토크나이저를 훈련(시킬 때)\n",
        "1. 어떤 기법을 사용할지?\n",
        "2. vocab size 결정 : 작으면 메모리 이득, 크면 의미 표현에 유리\n",
        "3. corpus를 정함 >> 통계적 분석, vocab 결정\n",
        "\n",
        "tokenizer에 vocab을 직접(수동) 추가하는 방법도 있음"
      ],
      "metadata": {
        "id": "IzjXIuV0chjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iS_8G75ddh0N",
        "outputId": "3a50b09a-8340-4fc7-a8ad-bbd18d7f1812"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PreTrainedTokenizer(name_or_path='monologg/koelectra-base-v3-discriminator', vocab_size=35000, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 문자열을 토큰으로 반환\n",
        "tokenizer.convert_tokens_to_ids('안녕')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zXkJ0xpaX3G",
        "outputId": "d721b6c2-bd3c-4c67-e307-b08fc365a0f5"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11655"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 넣은 숫자에 맞는 단어 출력. 없으면 UNK(unknown)으로 출력\n",
        "tokenizer.convert_ids_to_tokens([0,1,2,3, 155,156,30000,35001])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6fAlrkOacdv",
        "outputId": "aa3799d3-a98c-49b0-9cef-b7a9ad8b5a70"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '※', '‼', '킬로그램', '[UNK]']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 영어 모델 사용해보기\n",
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "UkzpGqeiajln"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 영어 모델이라 한글은 이상하게 표현됨(ex. ##ㅁㅁ)\n",
        "print(tokenizer.tokenize('안녕하세요'))\n",
        "print(tokenizer.tokenize('hello, world'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yn2xKnKGb1ZG",
        "outputId": "161e0dd7-8805-43e5-bf37-22059f7d342c"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ᄋ', '##ᅡ', '##ᆫ', '##ᄂ', '##ᅧ', '##ᆼ', '##ᄒ', '##ᅡ', '##ᄉ', '##ᅦ', '##ᄋ', '##ᅭ']\n",
            "['hello', ',', 'world']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMPmVTKFd8q3",
        "outputId": "2c66270b-f378-4917-ff2c-db174bb7975a"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30522"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 다국어 모델은 여러 언어에 대한 토큰화도 지원함\n",
        "multi_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
      ],
      "metadata": {
        "id": "cqCYBi58eouF"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 많이 쪼개서 처리할수록(=의미를 표현하지 못 할수록) 인식이 잘 안 됐다고 봄\n",
        "# ex. 안녕 --> 안, ##녕 --> 안녕이 vocab에 포함되어 있지 않음\n",
        "multi_tokenizer.tokenize('안녕하세요')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FK-62kb-e5jd",
        "outputId": "c142e33d-7aef-4e68-98a0-9165dfb04f6a"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['안', '##녕', '##하', '##세', '##요']"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "multi_tokenizer.vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoyANglue9n9",
        "outputId": "1c2cc2cc-d9a3-4299-b091-171038ecf00f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "119547"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizer\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
      ],
      "metadata": {
        "id": "cmFanS7UfFfu"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.convert_ids_to_tokens([11,222,33])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bblMGISJgWf9",
        "outputId": "bd24c54d-da36-4ab6-ed9b-31ffeb6e46ab"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Ġin', 'Ġdid', 'Ġhave']"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 초거대 언어 모델(GPT-3)\n",
        "- 다음 token(=단어)을 예측함\n",
        "- pre-training은 언어 모델을 만드는 과정\n",
        "- 라벨링 데이터를 pre-training로 돌림\n",
        "    - ex)오늘 날씨가 너무 좋다. // 긍정\n",
        "    - '오늘 날씨가 너무 좋다.'는 [긍정/부정] 문장이다.\n",
        "    - 정답 : 긍정"
      ],
      "metadata": {
        "id": "bXpHlN66j25K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3가지 종류 언어 모델**\n",
        "\n",
        "**1. Decoder-only 모델(Auto Regressive Model)(GPT 1 ~ 3, ...)**\n",
        "- few-show learning 사용\n",
        "- 단점 : 모델이 너무 큼큼\n",
        "- 점점 더 모델이 커지고 있음(모델이 크고, 데이터를 많이 넣을수록 성능이 좋음)\n",
        "\n",
        "**2. Encoder-only 모델(Auto Encoder Model)(BERT, ELECTRA, Albert, RoBERTA)**\n",
        "- input을 vector로 변환해줌. vector는 의미와 관련이 있음.\n",
        "- pre-training(사전학습+비지도학습) + fine-tuning(지도학습) 사용\n",
        "- fine-tuning은 downstream task(=추가훈련)(ex. 번역, 문장 분류, 질의응답 등)\n",
        "- Why?\n",
        "    1. 성능이 기존 방식보다 훨씬 좋다.\n",
        "    2. 소량의 데이터로도 학습이 훨씬 빨리 된다.\n",
        "    \n",
        "**3. Encoder-Decoder 모델(Transformer 모델과 구조는 동일함)(BART, T5, ...)**\n",
        "- 생성 관련 task가 사용이 가능함\n",
        "- pre-training(사전학습+비지도학습) + fine-tuning(지도학습)\n",
        "\n",
        "---\n",
        "\n",
        "- Encoder model : 생성 불가능, 추출 가능\n",
        "- Decoder model : 생성 가능\n",
        "- embedding : 훈련 됨, 특정 id나 token을 vector로 변환시켜줌\n",
        "- cf) encoding : 훈련 안 됨, 텍스트를 vector로 변환시켜줌\n",
        "- decoder에서 추론 시점에 encoder 토큰을 얼마나 참조하냐? -- attention\n",
        "- self-attention : 자기 자신을 entity로 해서 참조함\n",
        "- CLS token은 문장 전체의 맥락을 의미하는 token이라고 정의한다. CLS가 embedding 된 것은 문장의 의미를 나타낸다고 본다.\n",
        "    - CLS token 말고 문장 내 각 token의 평균값을 내는 방법을 사용하기도 함\n",
        "---\n",
        "## **한국어 pre-trained model**\n",
        "- koelectra 모델\n",
        "- klue-roberta 모델\n",
        "- korbert 모델\n",
        "\n",
        "---\n",
        "\n",
        "**text2image model**\n",
        "- bert >> 문장을 vector로 만들어준다\n",
        "- vector >> 이미지로 만든다"
      ],
      "metadata": {
        "id": "sG9G_8RwrZ5G"
      }
    }
  ]
}